{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I'm new to RL, so a tutorial Lux AI RL code was a little complicated for me.  \nThat's why I coded a light robot RL on my own simple env like Lux AI.  \nPhase1: reward for digging target, closing to target and pickup power.  \nPhase2: reward only for digging target.  \n","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade luxai_s2\n!pip install pettingzoo==1.12.0 gym==0.21.0 stable-baselines3\n!pip install --upgrade \"importlib_metadata<5.0\"\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-03-22T07:39:03.031034Z","iopub.execute_input":"2023-03-22T07:39:03.031408Z","iopub.status.idle":"2023-03-22T07:39:30.895525Z","shell.execute_reply.started":"2023-03-22T07:39:03.031375Z","shell.execute_reply":"2023-03-22T07:39:30.89432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /opt/conda/lib/python3.7/site-packages/luxai_s2/version.py\n__version__ = \"\"\n# this code above is used for Kaggle Notebooks\n# You might not need to run this but if you get an attribute error about the gym package, run it","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-03-22T07:39:30.897687Z","iopub.execute_input":"2023-03-22T07:39:30.898134Z","iopub.status.idle":"2023-03-22T07:39:30.905696Z","shell.execute_reply.started":"2023-03-22T07:39:30.898106Z","shell.execute_reply":"2023-03-22T07:39:30.904239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import importlib\nimport importlib_metadata\n# kaggle has 6.0.0 installed but we need version <5.0\n#importlib.reload(importlib_metadata) # commented out","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-03-22T07:39:30.907197Z","iopub.execute_input":"2023-03-22T07:39:30.907497Z","iopub.status.idle":"2023-03-22T07:39:30.918995Z","shell.execute_reply.started":"2023-03-22T07:39:30.90747Z","shell.execute_reply":"2023-03-22T07:39:30.917531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir logs","metadata":{"execution":{"iopub.status.busy":"2023-03-22T07:39:30.921585Z","iopub.execute_input":"2023-03-22T07:39:30.921937Z","iopub.status.idle":"2023-03-22T07:39:31.231699Z","shell.execute_reply.started":"2023-03-22T07:39:30.921908Z","shell.execute_reply":"2023-03-22T07:39:31.230394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from luxai_s2.env import LuxAI_S2\nluxenv = LuxAI_S2() # create the environment object\nlux_cfg = luxenv.state.env_cfg","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-03-22T07:39:31.233621Z","iopub.execute_input":"2023-03-22T07:39:31.234214Z","iopub.status.idle":"2023-03-22T07:39:31.23972Z","shell.execute_reply.started":"2023-03-22T07:39:31.234183Z","shell.execute_reply":"2023-03-22T07:39:31.238602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(lux_cfg)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T07:39:31.241342Z","iopub.execute_input":"2023-03-22T07:39:31.241653Z","iopub.status.idle":"2023-03-22T07:39:31.25387Z","shell.execute_reply.started":"2023-03-22T07:39:31.241615Z","shell.execute_reply":"2023-03-22T07:39:31.25282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gym\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\n# actions index\nACT_STAY = 0\nACT_MOVE_L = 1\nACT_MOVE_R = 2\nACT_MOVE_U = 3\nACT_MOVE_B = 4\nACT_DIG = 5\nACT_PICKUP_POWER = 6\nACTION_NUM = 7\n\nrobo_cfg = lux_cfg.ROBOTS['LIGHT'] # for Light robot\n\nclass LightEnv(gym.Env):\n    def __init__(self, saveVideo=False):\n        self.saveVideo = saveVideo\n        self.MAP_SIZE = lux_cfg.map_size\n        self.LEARN_MAP_SIZE = self.MAP_SIZE # change at each learning step\n        self.MAX_STEP = 200\n        self.LOW_POWER = 30\n        self.LEARN_PHASE = 1\n\n        self.WINDOW_SIZE = int(lux_cfg.map_size * 8)\n\n        # Actions\n        self.action_space = gym.spaces.Discrete(ACTION_NUM)\n        self.MOVE_MAP = np.array([[0, 0], [-1, 0], [1, 0], [0, -1], [0, 1]]) \n        \n        # Observation\n        #               [TargetX,          TargetY,          FactoryX,       FactoryY,       Power,                     OnFactory, TargetVal]\n        LOW = np.array( [-self.MAP_SIZE, -self.MAP_SIZE, -self.MAP_SIZE, -self.MAP_SIZE, 0,                         0, 0])\n        HIGH = np.array([self.MAP_SIZE,  self.MAP_SIZE,  self.MAP_SIZE,  self.MAP_SIZE,  robo_cfg.BATTERY_CAPACITY, 1, 100])\n        self.observation_space = gym.spaces.Box(low=LOW, high=HIGH)\n\n        self.reset()\n\n    def setRandomTargetPos(self):\n        tx = min(max(self.factory_position[0] + np.random.randint(-self.LEARN_MAP_SIZE, self.LEARN_MAP_SIZE),0),self.MAP_SIZE-1)\n        ty = min(max(self.factory_position[1] + np.random.randint(-self.LEARN_MAP_SIZE, self.LEARN_MAP_SIZE),0),self.MAP_SIZE-1)   \n        self.target_position = np.array([tx, ty])\n        while np.allclose(self.target_position, self.factory_position):\n            tx = min(max(self.factory_position[0] + np.random.randint(-self.LEARN_MAP_SIZE, self.LEARN_MAP_SIZE),0),self.MAP_SIZE-1)\n            ty = min(max(self.factory_position[1] + np.random.randint(-self.LEARN_MAP_SIZE, self.LEARN_MAP_SIZE),0),self.MAP_SIZE-1)\n            self.target_position = np.array([tx, ty])\n            \n    def reset(self):\n        if self.saveVideo:\n            video_name = f\"render{self.LEARN_PHASE}.webm\"\n            fourcc = cv2.VideoWriter_fourcc(*'VP90')\n\n            self.video = cv2.VideoWriter(video_name, fourcc, 20.0, (self.WINDOW_SIZE,self.WINDOW_SIZE))\n        \n        self.count = 0\n        self.factory_position = np.array([np.random.randint(0, self.MAP_SIZE), np.random.randint(0, self.MAP_SIZE)])\n        self.setRandomTargetPos()\n            \n        self.targetVal = 30\n        self.preTargetVal = 30\n\n        self.robo_position = self.factory_position\n\n        self.power = robo_cfg.INIT_POWER\n        self.prePower = self.power\n\n        # Create Observation\n        vec_tgt = self.robo_position - self.target_position\n        vec_fact = self.robo_position - self.target_position\n        observation = np.array([vec_tgt[0], vec_tgt[1],vec_fact[0],vec_fact[1],self.power,1,self.targetVal])\n        self.pre_target_distance = np.sum(np.abs(vec_tgt))\n        self.pre_fact_distance = np.sum(np.abs(vec_fact))\n        \n        self.last_act = ACT_STAY\n        self.done = False\n        return observation\n\n    def act_move(self, action_index):\n        action = self.MOVE_MAP[action_index]\n        if self.power >= 3:\n            self.robo_position = self.robo_position+action\n            self.robo_position = np.clip(self.robo_position, 0, self.MAP_SIZE-1)\n            self.power -= 3\n\n    def step(self, action_index):        \n        # Simulate MOVE Action\n        if ACT_MOVE_L<=action_index<= ACT_MOVE_B:\n            self.act_move(action_index)\n        \n        # Create the values for my work.\n        vec_tgt = self.robo_position - self.target_position\n        vec_fact = self.robo_position - self.factory_position\n        distance_target = np.sum(np.abs(vec_tgt))\n        distance_fact = np.sum(np.abs(vec_fact))\n        onTarget = (distance_target == 0)\n        onFactory = (distance_fact <= 1)\n\n        # Simulate DIG Action\n        if action_index == ACT_DIG:\n            if self.power >= robo_cfg.DIG_COST and onTarget:\n                self.power -= robo_cfg.DIG_COST\n                self.targetVal -= robo_cfg.DIG_RUBBLE_REMOVED\n                if self.targetVal <= 0:\n                    self.targetVal = 0\n                    \n        # Simulate PICKUP Action\n        if action_index == ACT_PICKUP_POWER:\n            if onFactory:\n                self.power = 150\n        \n        # Simulate Recharge\n        if (self.count % 50) < 30:\n            self.power += robo_cfg.CHARGE\n            if self.power > robo_cfg.BATTERY_CAPACITY:\n                self.power = robo_cfg.BATTERY_CAPACITY\n        if self.power < 0:\n            self.power = 0\n        \n        # Create observation\n        observation = np.array([vec_tgt[0], vec_tgt[1], vec_fact[0], vec_fact[1], self.power, 1 if onFactory else 0, self.targetVal])\n        reward = 0\n\n        # Calculate Reward\n        if self.LEARN_PHASE <= 1:\n            reward += (self.pre_target_distance - distance_target)*2 # Reward if the robot close to target.\n\n        reward += (self.preTargetVal - self.targetVal) * 3 # Reward if dig success\n\n        if self.power < self.LOW_POWER:\n            add_reward = self.pre_fact_distance - distance_fact  # Reward if robot close to factory when low power.\n            reward += add_reward*3\n            \n        # Reward if robot pick up a lot of power.\n        if self.LEARN_PHASE <= 1:\n            diffPower = self.power - self.prePower\n            if diffPower > 60:\n                reward += (diffPower - 60)/5\n            elif 2 < diffPower < 20:\n                reward -= 1\n        \n        # Done over max step\n        if self.count > self.MAX_STEP:\n            self.done = True\n\n        # Set parameters for next process.\n        if self.targetVal <= 0: # New target\n            self.setRandomTargetPos()\n            vec_tgt = self.robo_position - self.target_position\n            distance_target = np.sum(np.abs(vec_tgt))\n            self.pre_target_distance = distance_target\n            self.targetVal = 30\n            \n        self.last_act = action_index\n        self.pre_target_distance = distance_target\n        self.pre_fact_distance = distance_fact\n        self.prePower = self.power\n        self.preTargetVal = self.targetVal\n        self.count += 1\n        return observation, reward, self.done, {}\n\n    def render(self):\n        img = np.zeros((self.WINDOW_SIZE, self.WINDOW_SIZE, 3), np.uint8)\n        rate = self.WINDOW_SIZE/self.MAP_SIZE\n        \n        if ACT_MOVE_L <= self.last_act <= ACT_MOVE_B:\n            robo_color = (0,0,255)\n        elif self.last_act == ACT_DIG:\n            robo_color = (0,255,255)\n        elif self.last_act == ACT_PICKUP_POWER:\n            robo_color = (255,255,0)\n        else:\n            robo_color = (128,128,128)\n\n        # Draw a factory\n        cv2.rectangle(img, tuple(((self.factory_position-1)*rate).astype(np.int32)), tuple(((self.factory_position+1)*rate).astype(np.int32)), (255, 0, 0),thickness=-1)\n        cv2.circle(img,  tuple((self.target_position*rate).astype(np.int32)), 3, (0, 255, 0), thickness=-1) # Draw a target\n        cv2.circle(img,  tuple((self.robo_position*rate).astype(np.int32)), 3, robo_color, thickness=-1) # Draw a robot\n        cv2.putText(img, f\"P{self.power} T{self.targetVal}\", (12,12), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1, cv2.LINE_AA) # Draw status\n        self.video.write(img)\n        \n        if self.done:\n            self.video.release()\n","metadata":{"execution":{"iopub.status.busy":"2023-03-22T07:39:31.257719Z","iopub.execute_input":"2023-03-22T07:39:31.258105Z","iopub.status.idle":"2023-03-22T07:39:31.298292Z","shell.execute_reply.started":"2023-03-22T07:39:31.258076Z","shell.execute_reply":"2023-03-22T07:39:31.296613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training robot with RL","metadata":{}},{"cell_type":"code","source":"# Train\nfrom stable_baselines3 import DQN\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.monitor import Monitor\nlog_dir = \"./logs\"\n\nlightEnv = LightEnv()\nlightEnv.LEARN_MAP_SIZE = 6 # start from narrow size\nlightEnv.MAX_STEP = 200\nenv = Monitor(lightEnv, log_dir, allow_early_resets=True)\n\nmodel = PPO(\"MlpPolicy\", env, verbose=0, tensorboard_log=log_dir)\n# model = PPO.load(\"LightRobo1\", env=env, verbose=0, tensorboad_log=log_dir) # Comment out and rerun this cell, if you want to try additional training.\nprint('start learning') \nmodel.learn(total_timesteps=200000, progress_bar=True, log_interval=4)\nmodel.save(\"LightRobo1\")\nprint('finish learning')","metadata":{"execution":{"iopub.status.busy":"2023-03-22T07:39:31.300358Z","iopub.execute_input":"2023-03-22T07:39:31.300835Z","iopub.status.idle":"2023-03-22T07:42:50.171325Z","shell.execute_reply.started":"2023-03-22T07:39:31.300773Z","shell.execute_reply":"2023-03-22T07:42:50.170351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Result of leanring","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Reading monitor.csv\ndf = pd.read_csv('./logs/monitor.csv', names=['r', 'l','t'])\ndf = df.drop(range(2)) \n\n# Plot rewards\nx = range(len(df['r']))\ny = df['r'].astype(float)\nplt.plot(x, y)\nplt.xlabel('episode')\nplt.ylabel('reward')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-03-22T07:42:50.172819Z","iopub.execute_input":"2023-03-22T07:42:50.173394Z","iopub.status.idle":"2023-03-22T07:42:50.334676Z","shell.execute_reply.started":"2023-03-22T07:42:50.173355Z","shell.execute_reply":"2023-03-22T07:42:50.333926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = LightEnv(True)\nenv.LEARN_MAP_SIZE = 6\nenv.MAX_STEP = 500\n\nmodel = PPO.load(\"LightRobo1\")\nobs = env.reset()\n\n\nfor i in range(10000):\n    action, _states = model.predict(obs)\n    obs, rewards, dones, info = env.step(action)\n    env.render()\n    if dones:\n        break\n\nfrom IPython.display import Video\nVideo(\"render1.webm\")","metadata":{"execution":{"iopub.status.busy":"2023-03-22T07:42:50.337011Z","iopub.execute_input":"2023-03-22T07:42:50.337468Z","iopub.status.idle":"2023-03-22T07:42:53.980118Z","shell.execute_reply.started":"2023-03-22T07:42:50.337441Z","shell.execute_reply":"2023-03-22T07:42:53.979341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Phase 2 - remove inductive reward\nRemove target closer reward and pickup power reward. Because they are not our essentially purpose.","metadata":{}},{"cell_type":"code","source":"lightEnv.LEARN_MAP_SIZE = 7 #increase size\nlightEnv.MAX_STEP = 300 #increase step\nlightEnv.LEARN_PHASE = 2 #remove inductive reward\nenv = Monitor(lightEnv, log_dir, allow_early_resets=True)\n\nmodel = PPO.load(\"LightRobo1\", env=env, verbose=0, tensorboad_log=log_dir)\n#model = PPO.load(\"LightRobo2\", env=env, verbose=0, tensorboad_log=log_dir) # Comment out and rerun this cell, if you want to try additional training.\nprint('start learning') \nmodel.learn(total_timesteps=200000, progress_bar=True, log_interval=4)\nmodel.save(\"LightRobo2\")\nprint('finish learning')\n","metadata":{"execution":{"iopub.status.busy":"2023-03-22T07:42:53.981246Z","iopub.execute_input":"2023-03-22T07:42:53.98168Z","iopub.status.idle":"2023-03-22T07:46:12.688799Z","shell.execute_reply.started":"2023-03-22T07:42:53.981651Z","shell.execute_reply":"2023-03-22T07:46:12.687404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading monitor.csv\ndf = pd.read_csv('./logs/monitor.csv', names=['r', 'l','t'])\ndf = df.drop(range(2)) \n\n# Plot rewards\nx = range(len(df['r']))\ny = df['r'].astype(float)\nplt.plot(x, y)\nplt.xlabel('episode')\nplt.ylabel('reward')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T07:46:12.69074Z","iopub.execute_input":"2023-03-22T07:46:12.69206Z","iopub.status.idle":"2023-03-22T07:46:12.863286Z","shell.execute_reply.started":"2023-03-22T07:46:12.692Z","shell.execute_reply":"2023-03-22T07:46:12.862368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = LightEnv(True)\nenv.LEARN_MAP_SIZE = 6\nenv.MAX_STEP = 500\nenv.LEARN_PHASE = 2\n\nmodel = PPO.load(\"LightRobo2\")\nobs = env.reset()\n\nfor i in range(10000):\n    action, _states = model.predict(obs)\n    obs, rewards, dones, info = env.step(action)\n    env.render()\n    if dones:\n        break\n\nfrom IPython.display import Video\nVideo(\"render2.webm\")","metadata":{"execution":{"iopub.status.busy":"2023-03-22T07:46:12.864107Z","iopub.execute_input":"2023-03-22T07:46:12.864329Z","iopub.status.idle":"2023-03-22T07:46:16.562647Z","shell.execute_reply.started":"2023-03-22T07:46:12.864304Z","shell.execute_reply":"2023-03-22T07:46:16.561599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I think it learning fine.\nSometimes, the notebook view's movie might be bad learing case. Do extra train if you want see good learing case after copy.","metadata":{}}]}