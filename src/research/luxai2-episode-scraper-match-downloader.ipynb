{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**[Important]**  \nMost of this notebook is taken almost directly from [@robga](https://www.kaggle.com/robga)'s code in the [kore](https://www.kaggle.com/competitions/kore-2022) competition. \nhttps://www.kaggle.com/code/robga/kore-episode-scraper-match-downloader\n\nHe has provided us with his code for every simulation competitions. Thanks for the great notebooks.\n\nIn my notebook I have changed the way data is loaded. \nI use polars instead of pandas to improve loading speed because the meta kaggle dataset is huge and takes a long time to load.ã€€\nThis change reduces loading time by 1/3.\n\n---","metadata":{}},{"cell_type":"markdown","source":"This notebook downloads episodes using Kaggle's GetEpisodeReplay API and the [Meta Kaggle](https://www.kaggle.com/kaggle/meta-kaggle) dataset.\n\n**To run this notebook you WILL need to re-add the Meta Kaggle dataset. After opening your copy of the notebook, click \"+ Add data\" top right in the notebook editor.\n**\n\nMeta Kaggle is refreshed daily, but sometimes misses daily refreshes for a few days.\n\nWhy download replays?\n- Train your ML/RL model\n- Inspect the performance of yours and others agents\n- To add to your ever growing json collection \n\nOnly one scraping strategy is implemented: For each top scoring submission, download all missing matches, move on to next submission.\n\nOther scraping strategies can be implemented, but not here. Like download max X matches per submission or per team per day, or ignore certain teams or ignore where some scores < X, or only download some teams.\n\nTodo:\n- Add teamid's once meta kaggle add them. Edit: it's been a long time, it doesn't look like Kaggle is adding this.","metadata":{"papermill":{"duration":0.020728,"end_time":"2022-04-25T18:06:14.447266","exception":false,"start_time":"2022-04-25T18:06:14.426538","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# use to load meta kaggle \n!pip install polars","metadata":{"execution":{"iopub.status.busy":"2023-02-01T11:34:01.513572Z","iopub.execute_input":"2023-02-01T11:34:01.514486Z","iopub.status.idle":"2023-02-01T11:34:17.506611Z","shell.execute_reply.started":"2023-02-01T11:34:01.514317Z","shell.execute_reply":"2023-02-01T11:34:17.505025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport requests\nimport json\nimport datetime\nimport time\nimport glob\nimport collections\nimport polars as pl ","metadata":{"papermill":{"duration":0.039534,"end_time":"2022-04-25T18:06:14.503242","exception":false,"start_time":"2022-04-25T18:06:14.463708","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-01T11:34:17.509274Z","iopub.execute_input":"2023-02-01T11:34:17.510019Z","iopub.status.idle":"2023-02-01T11:34:17.599723Z","shell.execute_reply.started":"2023-02-01T11:34:17.509934Z","shell.execute_reply":"2023-02-01T11:34:17.598473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## You should configure these to your needs. Choose one of ...\n# 'hungry-geese', 'rock-paper-scissors', santa-2020', 'halite', 'google-football'\nCOMP = 'lux-ai-2022'\nMAX_CALLS_PER_DAY = 300 # Kaggle says don't do more than 3600 per day and 1 per second\nLOWEST_SCORE_THRESH = 1200","metadata":{"papermill":{"duration":0.027243,"end_time":"2022-04-25T18:06:14.550019","exception":false,"start_time":"2022-04-25T18:06:14.522776","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-01T11:34:17.60121Z","iopub.execute_input":"2023-02-01T11:34:17.60209Z","iopub.status.idle":"2023-02-01T11:34:17.607203Z","shell.execute_reply.started":"2023-02-01T11:34:17.602051Z","shell.execute_reply":"2023-02-01T11:34:17.606023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROOT =\"../working/\"\nMETA = \"../input/meta-kaggle/\"\nMATCH_DIR = '../working/'\n#base_url = \"https://www.kaggle.com/requests/EpisodeService/\"\nbase_url = \"https://www.kaggle.com/api/i/competitions.EpisodeService/\"\n    \nget_url = base_url + \"GetEpisodeReplay\"\nBUFFER = 1\nCOMPETITIONS = {\n    'lux-ai-2022': 45040,\n    'kore-2022': 34419,\n    'lux-ai-2021': 30067,\n    'hungry-geese': 25401,\n    'rock-paper-scissors': 22838,\n    'santa-2020': 24539,\n    'halite': 18011,\n    'google-football': 21723,\n}","metadata":{"papermill":{"duration":0.024347,"end_time":"2022-04-25T18:06:14.589576","exception":false,"start_time":"2022-04-25T18:06:14.565229","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-01T11:34:17.609643Z","iopub.execute_input":"2023-02-01T11:34:17.610014Z","iopub.status.idle":"2023-02-01T11:34:17.625554Z","shell.execute_reply.started":"2023-02-01T11:34:17.609972Z","shell.execute_reply":"2023-02-01T11:34:17.624494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n# switch from pandas to polars\nepisodes_df = pl.read_csv(META + \"Episodes.csv\")\nepisodes_df = episodes_df.filter(pl.col('CompetitionId')==COMPETITIONS[COMP]) \nepisodes_df = episodes_df.to_pandas()\nprint(f'Episodes.csv: {len(episodes_df)} rows after filtering for {COMP}.')\n\n# Filter Episodes.csv\n# data = pd.read_csv(META + \"Episodes.csv\", chunksize=1e6)\n# df_list = [] \n# for chunk in data:\n#     df_list.append(chunk[chunk['CompetitionId']==COMPETITIONS[COMP]])\n# episodes_df = pd.concat(df_list)\n# del data\n# del chunk\n# del df_list\n# print(f'Episodes.csv: {len(episodes_df)} rows after filtering for {COMP}.')","metadata":{"execution":{"iopub.status.busy":"2023-02-01T11:34:17.627091Z","iopub.execute_input":"2023-02-01T11:34:17.62767Z","iopub.status.idle":"2023-02-01T11:34:34.251761Z","shell.execute_reply.started":"2023-02-01T11:34:17.627634Z","shell.execute_reply":"2023-02-01T11:34:34.250784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nepagents_df = pl.read_csv(META + \"EpisodeAgents.csv\", dtypes={'Reward':pl.Float32})\nepagents_df = epagents_df.filter(pl.col(\"EpisodeId\").is_in(episodes_df['Id'].to_list()))\nepagents_df = epagents_df.to_pandas()\nepagents_df['InitialConfidence'] = epagents_df['InitialConfidence'].replace(\"\", np.nan).astype(float)\nepagents_df['InitialScore'] = epagents_df['InitialScore'].replace(\"\", np.nan).astype(float)\nprint(f'EpisodeAgents.csv: {len(epagents_df)} rows after filtering for {COMP}.')\n\n# Filter EpisodeAgents.csv\n# data = pd.read_csv(META + \"EpisodeAgents.csv\", chunksize=1e6)\n# df_list = [] \n# for chunk in data:\n#     df_list.append(chunk[chunk.EpisodeId.isin(episodes_df.Id)])\n# epagents_df = pd.concat(df_list)\n# del data\n# del chunk\n# del df_list\n# print(f'EpisodeAgents.csv: {len(epagents_df)} rows after filtering for {COMP}.')","metadata":{"papermill":{"duration":211.665971,"end_time":"2022-04-25T18:10:42.090856","exception":false,"start_time":"2022-04-25T18:07:10.424885","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-01T11:36:55.509774Z","iopub.execute_input":"2023-02-01T11:36:55.510267Z","iopub.status.idle":"2023-02-01T11:37:45.203515Z","shell.execute_reply.started":"2023-02-01T11:36:55.510223Z","shell.execute_reply":"2023-02-01T11:37:45.202303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare dataframes\n\nepisodes_df = episodes_df.set_index(['Id'])\nepisodes_df['CreateTime'] = pd.to_datetime(episodes_df['CreateTime'])\nepisodes_df['EndTime'] = pd.to_datetime(episodes_df['EndTime'])\n\nepagents_df.fillna(0, inplace=True)\nepagents_df = epagents_df.sort_values(by=['Id'], ascending=False)","metadata":{"papermill":{"duration":11.016333,"end_time":"2022-04-25T18:10:53.123759","exception":false,"start_time":"2022-04-25T18:10:42.107426","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get top scoring submissions# Get top scoring submissions\nmax_df = (epagents_df.sort_values(by=['EpisodeId'], ascending=False).groupby('SubmissionId').head(1).drop_duplicates().reset_index(drop=True))\nmax_df = max_df[max_df.UpdatedScore>=LOWEST_SCORE_THRESH]\nmax_df = pd.merge(left=episodes_df, right=max_df, left_on='Id', right_on='EpisodeId')\nsub_to_score_top = pd.Series(max_df.UpdatedScore.values,index=max_df.SubmissionId).to_dict()\nprint(f'{len(sub_to_score_top)} submissions with score over {LOWEST_SCORE_THRESH}')","metadata":{"papermill":{"duration":0.068584,"end_time":"2022-04-25T18:10:53.210316","exception":false,"start_time":"2022-04-25T18:10:53.141732","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get episodes for these submissions\nsub_to_episodes = collections.defaultdict(list)\nfor key, value in sorted(sub_to_score_top.items(), key=lambda kv: kv[1], reverse=True):\n    excl = []\n    if key not in excl: # we can filter subs like this\n        eps = sorted(epagents_df[epagents_df['SubmissionId'].isin([key])]['EpisodeId'].values,reverse=True)\n        sub_to_episodes[key] = eps\ncandidates = len(set([item for sublist in sub_to_episodes.values() for item in sublist]))\nprint(f'{candidates} episodes for these {len(sub_to_score_top)} submissions')","metadata":{"papermill":{"duration":0.036445,"end_time":"2022-04-25T18:10:53.263034","exception":false,"start_time":"2022-04-25T18:10:53.226589","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"global num_api_calls_today\nnum_api_calls_today = 0\nall_files = []\nfor root, dirs, files in os.walk(MATCH_DIR, topdown=False):\n    all_files.extend(files)\nseen_episodes = [int(f.split('.')[0]) for f in all_files \n                      if '.' in f and f.split('.')[0].isdigit() and f.split('.')[1] == 'json']\nremaining = np.setdiff1d([item for sublist in sub_to_episodes.values() for item in sublist],seen_episodes)\nprint(f'{len(remaining)} of these {candidates} episodes not yet saved')\nprint('Total of {} games in existing library'.format(len(seen_episodes)))","metadata":{"papermill":{"duration":0.032705,"end_time":"2022-04-25T18:10:53.314859","exception":false,"start_time":"2022-04-25T18:10:53.282154","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_info_json(epid):\n    \n    create_seconds = int((episodes_df[episodes_df.index == epid]['CreateTime'].values[0]).item()/1e9)\n    end_seconds = int((episodes_df[episodes_df.index == epid]['CreateTime'].values[0]).item()/1e9)\n\n    agents = []\n    for index, row in epagents_df[epagents_df['EpisodeId'] == epid].sort_values(by=['Index']).iterrows():\n        agent = {\n            \"id\": int(row[\"Id\"]),\n            \"state\": int(row[\"State\"]),\n            \"submissionId\": int(row['SubmissionId']),\n            \"reward\": float(row['Reward']),\n            \"index\": int(row['Index']),\n            \"initialScore\": float(row['InitialScore']),\n            \"initialConfidence\": float(row['InitialConfidence']),\n            \"updatedScore\": float(row['UpdatedScore']),\n            \"updatedConfidence\": float(row['UpdatedConfidence']),\n            \"teamId\": int(99999)\n        }\n        agents.append(agent)\n\n    info = {\n        \"id\": int(epid),\n        \"competitionId\": int(COMPETITIONS[COMP]),\n        \"createTime\": {\n            \"seconds\": int(create_seconds)\n        },\n        \"endTime\": {\n            \"seconds\": int(end_seconds)\n        },\n        \"agents\": agents\n    }\n\n    return info","metadata":{"papermill":{"duration":0.032426,"end_time":"2022-04-25T18:10:53.364343","exception":false,"start_time":"2022-04-25T18:10:53.331917","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def saveEpisode(epid):\n    # request\n    re = requests.post(get_url, json = {\"episodeId\": int(epid)})\n        \n    # save replay\n    with open(MATCH_DIR + '{}.json'.format(epid), 'w') as f:\n        f.write(re.json()['replay'])\n\n    # save match info\n    info = create_info_json(epid)\n    with open(MATCH_DIR +  '{}_info.json'.format(epid), 'w') as f:\n        json.dump(info, f)\n","metadata":{"papermill":{"duration":0.027178,"end_time":"2022-04-25T18:10:53.408465","exception":false,"start_time":"2022-04-25T18:10:53.381287","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r = BUFFER;\n\nstart_time = datetime.datetime.now()\nse=0\nfor key, value in sorted(sub_to_score_top.items(), key=lambda kv: kv[1], reverse=True):\n    if num_api_calls_today<=MAX_CALLS_PER_DAY:\n        print('')\n        remaining = sorted(np.setdiff1d(sub_to_episodes[key],seen_episodes), reverse=True)\n        print(f'submission={key}, LB={\"{:.0f}\".format(value)}, matches={len(set(sub_to_episodes[key]))}, still to save={len(remaining)}')\n        \n        for epid in remaining:\n            if epid not in seen_episodes and num_api_calls_today<=MAX_CALLS_PER_DAY:\n                saveEpisode(epid); \n                r+=1;\n                se+=1\n                try:\n                    size = os.path.getsize(MATCH_DIR+'{}.json'.format(epid)) / 1e6\n                    print(str(num_api_calls_today) + f': saved episode #{epid}')\n                    seen_episodes.append(epid)\n                    num_api_calls_today+=1\n                except:\n                    print('  file {}.json did not seem to save'.format(epid))    \n                if r > (datetime.datetime.now() - start_time).seconds:\n                    time.sleep( r - (datetime.datetime.now() - start_time).seconds)\n            if num_api_calls_today>(min(3600,MAX_CALLS_PER_DAY)):\n                break\nprint('')\nprint(f'Episodes saved: {se}')","metadata":{"papermill":{"duration":308.645811,"end_time":"2022-04-25T18:16:02.071799","exception":false,"start_time":"2022-04-25T18:10:53.425988","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.089397,"end_time":"2022-04-25T18:16:02.252047","exception":false,"start_time":"2022-04-25T18:16:02.16265","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}