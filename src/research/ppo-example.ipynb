{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"It is important to say that this notebook was taken from lux code for \"stable baselines 3\":\nhttps://github.com/Lux-AI-Challenge/Lux-Design-S2/blob/main/examples/sb3.py\nI made only several changes.\nI hope it will be useful for those who don't know where to find RL model examples.","metadata":{}},{"cell_type":"code","source":"!pip install stable-baselines3\n!pip install --upgrade luxai_s2\n!pip install gym==0.21.0\n!pip install importlib-metadata==4.12.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Please, restart your kernel after loading libraries","metadata":{}},{"cell_type":"markdown","source":"Here is the main part of the code for PPO model of stable baselines 3:","metadata":{}},{"cell_type":"code","source":"import copy\nimport os\nimport os.path as osp\nimport gym\nimport numpy as np\nimport torch as th\nimport torch.nn as nn\nfrom gym import spaces\nfrom gym.wrappers import TimeLimit\nfrom stable_baselines3.common.callbacks import BaseCallback, EvalCallback\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.utils import set_random_seed\nfrom stable_baselines3.common.vec_env import (\n    DummyVecEnv,\n    SubprocVecEnv,\n    VecCheckNan,\n    VecVideoRecorder,\n)\nfrom stable_baselines3.ppo import PPO\n\nfrom luxai_s2.state import ObservationStateDict, StatsStateDict, create_empty_stats\nfrom luxai_s2.utils.heuristics.factory import build_single_heavy\nfrom luxai_s2.utils.heuristics.factory_placement import place_near_random_ice\nfrom luxai_s2.wrappers import (\n    SB3Wrapper,\n    SimpleSingleUnitDiscreteController,\n    SingleUnitObservationWrapper,\n)\n#os.chdir('/kaggle/input')\n\nclass CustomEnvWrapper(gym.Wrapper):\n    def __init__(self, env: gym.Env) -> None:\n        \"\"\"\n        Adds a custom reward and turns the LuxAI_S2 environment into a single-agent environment for easy training\n        \"\"\"\n        super().__init__(env)\n        self.prev_step_metrics = None\n\n    def step(self, action):\n        agent = \"player_0\"\n        opp_agent = \"player_1\"\n\n        opp_factories = self.env.state.factories[opp_agent]\n        for k in opp_factories:\n            factory = opp_factories[k]\n            factory.cargo.water = 1000 # set enemy factories to have 1000 water to keep them alive the whole around and treat the game as single-agent\n\n        action = {agent: action}\n        obs, reward, done, info = super().step(action)\n\n        # this is the observation seen by both agents\n        shared_obs: ObservationStateDict = self.env.prev_obs[agent]\n        done = done[agent]\n\n        # we collect stats on teams here:\n        stats: StatsStateDict = self.env.state.stats[agent]\n        \n        # compute reward\n        # we simply want to encourage the heavy units to move to ice tiles\n        # and mine them and then bring them back to the factory and dump it\n        # as well as survive as long as possible\n\n        factories = shared_obs[\"factories\"][agent]\n        factory_pos = None\n        for unit_id in factories:\n            factory = factories[unit_id]\n            # note that ice converts to water at a 4:1 ratio\n            factory_pos = np.array(factory[\"pos\"])\n            break\n        units = shared_obs[\"units\"][agent]\n        unit_deliver_ice_reward = 0\n        unit_move_to_ice_reward = 0\n        unit_overmining_penalty = 0\n        penalize_power_waste = 0\n\n        ice_map = shared_obs[\"board\"][\"ice\"]\n        ice_tile_locations = np.argwhere(ice_map == 1)\n\n        def manhattan_dist(p1, p2):\n            return abs(p1[0] - p2[0]) + abs(p1[1] - p2[1])\n\n        unit_power = 0\n        for unit_id in units:\n            unit = units[unit_id]\n            if unit[\"unit_type\"] == \"HEAVY\":\n                pos = np.array(unit[\"pos\"])\n                ice_tile_distances = np.mean((ice_tile_locations - pos) ** 2, 1)\n                closest_ice_tile = ice_tile_locations[np.argmin(ice_tile_distances)]\n                dist_to_ice = manhattan_dist(closest_ice_tile, pos)\n                unit_power = unit[\"power\"]\n                if unit[\"cargo\"][\"ice\"] < 20:\n\n                    dist_penalty = min(\n                        1.0, dist_to_ice / (10)\n                    )  # go beyond 12 squares manhattan dist and no reward\n                    unit_move_to_ice_reward += (\n                        1 - dist_penalty\n                    ) * 0.1  # encourage unit to move to ice\n                else:\n                    if factory_pos is not None:\n                        dist_to_factory = manhattan_dist(pos, factory_pos)\n                        dist_penalty = min(1.0, dist_to_factory / 10)\n                        unit_deliver_ice_reward = (\n                            0.2 + (1 - dist_penalty) * 0.1\n                        )  # encourage unit to move back to factory\n                if action[agent] == 15 and unit[\"power\"] < 70:\n                    # penalize the agent for trying to dig with insufficient power, which wastes 10 power for trying to update the action queue\n                    penalize_power_waste -= 0.005\n\n        # save some stats to the info object so we can record it with our SB3 logger\n        info = dict()\n        metrics = dict()\n        metrics[\"ice_dug\"] = (\n            stats[\"generation\"][\"ice\"][\"HEAVY\"] + stats[\"generation\"][\"ice\"][\"LIGHT\"]\n        )\n        metrics[\"water_produced\"] = stats[\"generation\"][\"water\"]\n        metrics[\"action_queue_updates_success\"] = stats[\"action_queue_updates_success\"]\n        metrics[\"action_queue_updates_total\"] = stats[\"action_queue_updates_total\"]\n\n        metrics[\"unit_deliver_ice_reward\"] = unit_deliver_ice_reward\n        metrics[\"unit_move_to_ice_reward\"] = unit_move_to_ice_reward\n\n        info[\"metrics\"] = metrics\n\n        reward = (\n            0\n            + unit_move_to_ice_reward\n            + unit_deliver_ice_reward\n            + unit_overmining_penalty\n            + metrics[\"water_produced\"] / 10 + penalize_power_waste\n        )\n        reward = reward\n        if self.prev_step_metrics is not None:\n            ice_dug_this_step = metrics[\"ice_dug\"] - self.prev_step_metrics[\"ice_dug\"]\n            water_produced_this_step = (\n                metrics[\"water_produced\"] - self.prev_step_metrics[\"water_produced\"]\n            )\n            # reward += ice_dug_this_step # reward agent for digging ice\n            # reward += water_produced_this_step * 100 # reward agent even more producing water by delivering ice back to base\n        self.prev_step_metrics = copy.deepcopy(metrics)\n        return obs[\"player_0\"], reward, done, info\n\n    def reset(self, **kwargs):\n        obs = self.env.reset(**kwargs)[\"player_0\"]\n        self.prev_step_metrics = None\n        return obs\n\n\nclass Parse_args:\n    def __init__(self):\n        self.max_episode_steps = 100\n        self.seed = 13\n        self.n_envs = 8\n        self.total_timesteps = 2000000\n        self.log_path = \"logs\"\n        self.model_path = '/kaggle/working/latest_model'\n        self.eval = False\n    \n    \ndef make_env(env_id: str, rank: int, seed: int = 0, max_episode_steps=100):\n    def _init() -> gym.Env:\n        # verbose = 0\n        # collect stats so we can create reward functions\n        # max factories set to 2 for simplification and keeping returns consistent as we survive longer if there are more initial resources\n        env = gym.make(env_id, verbose=0, collect_stats=True, MAX_FACTORIES=2)\n\n        # Add a SB3 wrapper to make it work with SB3 and simplify the action space with the controller\n        # this will remove the bidding phase and factory placement phase. For factory placement we use\n        # the provided place_near_random_ice function which will randomly select an ice tile and place a factory near it.\n        env = SB3Wrapper(\n            env,\n            controller=SimpleSingleUnitDiscreteController(env.state.env_cfg),\n            factory_placement_policy=place_near_random_ice,\n            heuristic_policy=build_single_heavy,\n        )\n        env = SingleUnitObservationWrapper(\n            env\n        )  # changes observation to include a few simple features\n        env = CustomEnvWrapper(env)  # convert to single agent and add our reward\n        env = TimeLimit(\n            env, max_episode_steps=max_episode_steps\n        )  # set horizon to 100 to make training faster. Default is 1000\n        env = Monitor(env)  # for SB3 to allow it to record metrics\n        env.reset(seed=seed + rank)\n        set_random_seed(seed)\n        return env\n\n    return _init\n\n\nenv_id = \"LuxAI_S2-v0\"\n\nfrom collections import defaultdict\n\n\nclass TensorboardCallback(BaseCallback):\n    def __init__(self, tag: str, verbose=0):\n        super().__init__(verbose)\n        self.tag = tag\n\n    def _on_step(self) -> bool:\n        c = 0\n        \n        for i, done in enumerate(self.locals[\"dones\"]):\n            if done:\n                info = self.locals[\"infos\"][i]\n                c += 1\n                for k in info[\"metrics\"]:\n                    stat = info[\"metrics\"][k]\n                    self.logger.record_mean(f\"{self.tag}/{k}\", stat)\n        return True\n\n\ndef evaluate(args, model,):\n    model = model.load(args.model_path)\n    video_length = 1000  # default horizon\n    eval_env = SubprocVecEnv([make_env(env_id, i, max_episode_steps=1000) for i in range(args.n_envs)])\n    eval_env = VecVideoRecorder(\n        eval_env,\n        osp.join(args.log_path, \"eval_videos\"),\n        record_video_trigger=lambda x: x == 0,\n        video_length=video_length,\n        name_prefix=f\"evaluation_video\",\n    )\n    eval_env.reset()\n    out =evaluate_policy(model, eval_env, render=False, deterministic=False)\n    print(out)\n\ndef train(args, model: PPO,):\n    eval_env = SubprocVecEnv([make_env(env_id, i, max_episode_steps=1000) for i in range(4)])\n    video_length = 1000\n    eval_env = VecVideoRecorder(\n        eval_env,\n        osp.join(args.log_path, \"eval_videos\"),\n        record_video_trigger=lambda x: x == 0,\n        video_length=video_length,\n        name_prefix=f\"evaluation-{env_id}\",\n    )\n    eval_callback = EvalCallback(\n        eval_env,\n        best_model_save_path=osp.join(args.log_path, \"models\"),\n        log_path=osp.join(args.log_path, \"eval_logs\"),\n        eval_freq=24_000,\n        deterministic=False,\n        render=False,\n    )\n    model.learn(\n        args.total_timesteps,\n        callback=[TensorboardCallback(tag=\"train_metrics\"), eval_callback],\n    )\n    \n    model.save(args.log_path, \"latest_model\")","metadata":{"execution":{"iopub.status.busy":"2023-01-31T18:27:03.675333Z","iopub.execute_input":"2023-01-31T18:27:03.675808Z","iopub.status.idle":"2023-01-31T18:27:05.335711Z","shell.execute_reply.started":"2023-01-31T18:27:03.675722Z","shell.execute_reply":"2023-01-31T18:27:05.33471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is the main function which creates your PPO model:","metadata":{}},{"cell_type":"code","source":"def main(args):\n    print(\"Training with args\", args)\n    set_random_seed(args.seed)\n    env = SubprocVecEnv([make_env(env_id, i, max_episode_steps=args.max_episode_steps) for i in range(args.n_envs)])\n    env.reset()\n    rollout_steps = 4_000\n    policy_kwargs = dict(net_arch=(128, 128))\n    model = PPO(\n        \"MlpPolicy\",\n        env,\n        n_steps=rollout_steps // args.n_envs,\n        batch_size=800,\n        learning_rate=1e-3,\n        policy_kwargs=policy_kwargs,\n        verbose=1,\n        n_epochs=3,\n        target_kl=0.07,\n        gamma=0.97,\n        device=th.device('cuda'),\n        tensorboard_log=osp.join(args.log_path),\n    )\n    if args.eval:\n        evaluate(args, model)\n    else:\n        train(args, model)\n\nif __name__ == \"__main__\":\n    parse_args = Parse_args()\n    main(parse_args)","metadata":{"execution":{"iopub.status.busy":"2023-01-31T18:27:06.157707Z","iopub.execute_input":"2023-01-31T18:27:06.158389Z","iopub.status.idle":"2023-01-31T18:46:02.722277Z","shell.execute_reply.started":"2023-01-31T18:27:06.158348Z","shell.execute_reply":"2023-01-31T18:46:02.720648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"My code is still raw but I'm ready to hear your advice for improving it or you can improve it on your own.","metadata":{}}]}